{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9e547f",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239464-lesson-1-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "## Review\n",
    "\n",
    "In module 2, covered a few ways to customize graph state and memory.\n",
    " \n",
    "We built up to a Chatbot with external memory that can sustain long-running conversations. \n",
    "\n",
    "## Goals\n",
    "\n",
    "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways. \n",
    "\n",
    "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b430d92-f595-4322-a56e-06de7485daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = os.environ[\"GEMINI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0682fc",
   "metadata": {},
   "source": [
    "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEICAIAAAA4AWNJAAAQAElEQVR4nOzdB1gU194G8LNL770JCIoajSgkYgETUUFjvDeC2EtsMRp7wWuC2IKoqIgliT0qqCgqxpJrLJ+9xB6M3SgSFQWkSIel7PfHyd0QXVaFxbML7+/h4Zk9MzvM7Oy8c8oyqymVShkAwDunyQAAeED6AAAfSB8A4APpAwB8IH0AgA+kDwDwoZz0efogP+2JJC+nhNVumtoiQ2NNCzttizo6TOWVlkgf38t/niIpyCtlAEqirSs2NNW0dtQ2NtdWvKSoip/3KZKU7l39hIlERmZa+oa1vSalpS1OfVpAr6iJuebH3S2ZCktKKDi+45m2ntjGWV9ags98gdJo64iTH+aLRKyOi+6HHc0ULFml9KHo2bPyiVt7C1tnPQblXDmaKpKydgEqGkApjwpO/pTWsZ8dxSUDqB6ndiU5NNRr/pFJRQtU6c1HtR5Ej1wfdrQsLpJePprBVE9xUWns8sRPBtsjeqBafRxg++B6bvy1nIoWqPz7j/p6qMGF6KmIW3vz62cypaUq16i5cjTDzduMAVQ/qp3EncisaG7l04e6mY3NtBhUQFtXQ1rKsp8XMxWT8khiYqXNAKqfmY12WTWlApVPHxrh0qv13cyK6Rlp5mWp3DhgfnaJngEOHLwLYrFIR09ckCv/LMC7EAD4QPoAAB9IHwDgA+kDAHwgfQCAD6QPAPCB9AEAPpA+AMAH0gcA+ED6AAAfSB8A4APpAwB84A4vUKPE7trm06kVg9eZNXtq4JRRjCs1Tp9vQ77Z/8se9va69+j05Gkig5ro/Saunw8czkCe8qdMu3Y+nTp1ZVypccvrzp2bLVt6sreUlPT0+XNVvOUgKEWTJq70w0Ce8qeMT8dPGG9qkD7nzp+JiYm6feeGubmlq6vbiOHjLCwsO/h40KxF4XNWrlqyb8/xnJycHTs3X7j4a0LCfQtzSy8v72FDR+nq6rIXNUwNDQ0bG7ttMVFDBo/cGLmaCgcM9Gvb1js0ZDGDNyD3ENy6fWP0mMErfohs0ripsNjAz/3plR89atJPu7dv2rxuYdj3wTMmpaWlOjnVC5wUTKE/P2xmcUlxSw/PyZOmmZqW3V/RP8CXDsrjxw9jd22lEs82H48dM2Ve2IwzZ044OjoN7D+sc+d/0WJveHy/nb3w2bOUFSsjjhy+QGuYPjPwpR3ZFLnLwaFucXHxj+tXnDt/OiUlydXVvbtf7zZtPnrti5CVnbV69TKqO5iYmHq0aP3l8HE2NrZUnpeXF7F0XlzcpezsLGen+p9+6ufv14vKHzy4P2x4H3p9oqM3nD5z3MrKukP7ziO+HFdQUOAf4DN40IiBA4YJay4pKenm38GvWy+am56eRtt//cZVWoySYtDA4fQ6sBctyuitGyZNDKL99ffvPW7MlIcPEzZsXBV39bJUKm3atHnf3oOaNXMX/u7efTuv/HYxKekJbU/Xrv5+3XpS+UunDK0nJyd7cfjKSuwCveBMGVS95XX3j9tB0yZ88EHLjet3jh839f79uwsWzqbyA/vP0O//TJlBryNN7PqJjs3GPr0/nzd36ciRE46fOBwZtUZYg5aWVvyDe/Qzd04EHYb5c5dS4ZbNexA9b6iiQ6AAveb0zt4YtTp84Qo6QEVFRfPCZv5yYO+6tdu2bNpz7XpczPZNsiW3xUTWret88Jezw78YQ8tMmjzCp2OXwwfPdWjfadHiOdk52eyNj2/zZh/ItoFSMmLxKtmPi0tDWxs7CwsrmrX8u4U7Y6O7+/eJ3rLPu53PrG+nnjh5RPEeUWB9EzQ+Ne0ZrWrc2P+kPEv+Ztp4KqRZNPHkyeM5IYu3b9tPzZllyxdQLgsbRr8XR4T6+HQ5dODX4KDQ7Ts2Hzt+2MDAgEL21KmjspVfunyezn/aa4qhSYEjKVAmTZy2fl2Mmak55Xvik8e0jLa2dl5e7t69O4O+CaG4lEgkEyePoBRYEPbd4kUrNTU0g6dPosCiJX9YsfjixV8njP86bP5yih7aHrp4sFdOmfLedheYkqh63ef6tTi6xNFVQiwW06Wm8Xvv0/vs1cV69xpIbyO6xv71rOtXL1w8O3LEeJoWiUR0EVi1YpNwqYS39YaH4CWUOHR5F67brVu1pfhYvnSdubkFPXR3a0ERJluyYYPG3T7rQRPtvTuFLw6lyzjlDj2ky2zUpnUP/3xAJZU4vlRD+cDdQ5jes3dnYuKj75dv0NPTKywsPHjo5/79hgh/tOunfrS2qE1raf0KdocqSrduXY/csJOCkh7SftF5SPUUeimuXYujpKhXz4XKB/Qfev7CGUrGsHnLhCd6t/Nt7+1LE25uH9axs79795avTxdvb9/QucFPk57Y2dahWadPH3N2rk/5GBd3mWo0VB/58IOWVD7qq4lnzp6IjY2m0KfdpHDp23ewMOv+/T8yMtJ7BPRr1LAxPZw1M+zq71eENJwxYz7llLBmegUOHNhLr1Wb1m0r3rUzldgFpgyqnj6uzdzpRQ8Knkh1XU/Pdg72jrK3VHkU0hcv/Rq2YNa9+3eFY2BmZi6b61S3HqKn0t7wELyK6vDChL6+Ph0OIXqInp5+ckqSbDHhfCZUKSh7lrOLbDH6TW0BVrXje+/e3e9/CA+eFkqnNz2kk4cqDtT6ky1AaUh1rsysTBPjCr/7hc522gvZptI5P31aKE0cOXqA/rRw3v5vVhMq/PthoyayaUNDo5wXVbm2Xt46OjpU/aFUpXYT1bxogsqpVkh7KuQLexGstG0UK7I1NH7vr0YutR+poRq2cHYn3660DFX0/j4oUumuXdsoQR49+lMosLOzZxV78OBeJXZBKVQ9fegwUwXy5Mkja9Z+t2LlkhYftqJuAnqtX1qM5u7fv5vq5PSuouvzuh9/KD8cpq2jBt8sqrLe8BC8ik4eudMKFmNldwKW0xtQ6eNLnTXTZ06mLhXh6s3KupDKTp5xE754acmM9DQF6ZObm6OjIyfgqFdLV/cfX+tCIZWfn8cU7g6d7V6e7U6dPkahQ/UOSlgKEWHbqM4odNDICB1kAmp/CRMUXsuWrP3v/t3UhKQ+rDp1HIYMGkFjWKWlpd9Mm1BUJPly+Fh3dw8jQ6NX91Qpu6AUatDr3LqVF/0MHfLV5cvnqW9yWvDEXbH/aHnS1WPfz7E9e/T/97+6CyVKjGdgb3AIBNSjzKpBVY5vaOg06pCmJoysxMKyrOsncHKwvb1j+SWtrW0VrEdf34BOSDq3XzoVqb5WUPCP72zIzcu1fNG7pFj79p2o35fO/JOnjlLTUujApr58ahvODV1SfkkNsfwuXqqI0X7RQbly5QLV3ahnzcm5Pm3h7ds3whetoIuEsBi9VlaW1gq2pNK7UHWq3utMLeHzF87ShKWl1Sef/HvM6EDqhkxKflp+Gbpc5OfnW/7vJaZ69dlfTzJQkooOgY52WY1DdpGkYanU1GesGlT6+FJHNfXLhMxeVH6MxsG+rs6LuhI1VYQfaiFS240u+ApWRb1d1Py8c/eW8JB6Z6jTl5pj7zUqK//j3h3ZktQ95FyuFVMR6nim0566k44eO0j9zUKhi0sj2lPKQdm2UXQ2aPDeq0+nDaDEYUI1yqvd7FkLNDU1qVGZmfmcCmVxk5AQTz+Kt6TSu1B1qp4+NPQ4+9up+37eReO1N29dp85LOgdo8ILeQDT+d+nSud/iLtHliK4DdDBodIBe/YXhIc1c3ak2m5ub++oKHV803Y8fP0xrY/AGKjoE1PNKFXtqAVHdhPpiwhbOMjIyZtWAmhtvfnxlrl69snbd9337DKIAojeJ8JOSkkwpQy1H6mamJg8FGfW5TJk6eumyMMXb4OHRhupKa9Ysp+bSxUvnaPlnKcnUC96qlRe1eiIi5t6+c5M6oakRRKdun16fs9eh/h0vL28aw6I9krUKqcJCKwwPn5OcnETlu/fs+GrU5wdepMxLsrIyFy4KWblq6ePER9S/syV6Ax0C16ZulKQUQzSkSE1OSqjvvl/U0qONcLUuf8oIfWeCSu9C1al6y4saxvSmp17DiCXz6F3YscMnSyLW0OvLyjrnh23YuIr687dG/zwjeB4NNA4Z2pMuBaNHTaYW74ULZ7v38I3cGPvSCu3rOHT55DN6Ih2qJRGrGbyOgkNAwys0OtvRtyXl0cgRE+i9S0nEqsGbH18ZGthiZcPPEeULx46Z0iOgL0US1TKit22kNouBgWHT95sHBk5XvAG0v+ELV8xfMHPmrP/QQ0/Pj+fPWya8CKEhi1etXkpD4/Ti1K/fcE5IuPC5m9dq3843+PBkSofyPejz5y7duy82JDTo5s1rlO++vp8GBPR99bnU7zZ50rSNkatp6I0e0oBAxOJVNHBG09S/TiNWfv4dKS6Dg+akpafOmDll8NCeNGBX/pQpv2uV3oUqElX67XLhYLqkoOz7ghlUYP+Pj70DLG2dVWu4bceSxy06WVo5YhAQ3oWYRfEDg5x0DeT0XuF/3AGAj3eXPp91ay+3vKSkhDpuKhqR3bxpt4mJKasG1OynsRu5s6g7gJrlcjeJhhW+X76eQY2j4P3AqvN9WJu9u/RZsyaavb3qO+TUsq1ok3Jzc6g7QO4sTQ3UFmsmBe8HVp3vw9rs3Z1Lwke/VYoKbhJwhPfDO4YrOQDwgfQBAD6QPgDAB9IHAPhA+gAAH0gfAOAD6QMAfCB9AIAPpA8A8FH5+/voGmiUllbL7RRqDC1tkY6uyt1Bychcs7iolAG8E7oGmlo68s+Cyp8bFrbaKQ8LGFSAzvDkhwVmttpMxRiba6Y+KWQA1S89uVAsZhqa8v+HvPLpU8dFt1hSkpNZxECe+N+zXT2r5V5/VdSklfHDWzkMoPrF/57V1KvCs6Dy6SMSiT4danfmp+SCvBIG/5RwM5vO8I+7v4tbc78tMxvtFj6mx3c8ZQDV6fdT6dISqdvHFd4eQFTFW2FmphZtX/KoXjMjUyttPcPa3oct1mAZyRJJfvHzZ5JuI+uIxSKmqu5cyr5+NtPMVtemri4Tqe52gtqhdlZqYoEkv6SkqLTTQBsFS4qUciPeG+cyUx4W5mbxrARJJJLExMR69eoxfvSMNPT0xdZ1dRq4GTGVR1eO+Gs5WenF2RnV8k04UDsZmmrqGYht6+k6NTZQvKSomm4D/u4lJCQEBgbGxsYyAFAH+LwPAPCB9AEAPpA+AMAH0gcA+ED6AAAfSB8A4APpAwB8IH0AgA+kDwDwgfQBAD6QPgDAB9IHAPhA+gAAH0gfAOAD6QMAfCB9AIAPpA8A8IH0AQA+kD4AwAfSBwD4QPoAAB9IHwDgA+kDAHzUnPQRiUQ2NjYMANREzUkfqVSanJzMAEBNoOUFAHwgfQCAD6QPAPCB9AEAPpA+AMAH0gcA+ED6AAAfSB8A4APpAwB8IH0AgA+kDwDwgfQBAD6QPgDAB9IHAPhA+gAAHyKpVMrU2cCBA58/f66hoVFYWJienm5jYyMWi/Pz8w8dOsQAQIWJmZrr2gP+NwAADy5JREFU1asXhU5iYmJqamppaenTp09pmsKIAYBqU/v08fPzq1u3bvkSqs15enoyAFBtap8+pHfv3jo6OrKH1PgaPHgwAwDVVhPSJyAgwN7eXvawbdu2Tk5ODABUW01IH9K/f3+h+uPg4DBo0CAGACqvhqSPv78/5Q57UfFxdHRkAKDyXv95n6LC0rSnkrycEqba/DuPPHDgwMctesZfz2UqTCRiJhZaptZaYrGIAdRir/m8z8ldz+7F5RiYaOoZ4nOJyqFvrJH0IF/XUMPVy7ixhzEDqK0Upc8vG56a2ek29TRjoGylpdITO5IauBm83xoBBLVUhelzeEuyqY1O45amDKrN0a1P3m9j3NDdkAHUPvJ7nZMfFRTklyJ6qpuXn82105kMoFaSnz7pTyWaWjVkOEyV6eprpD8tzFf5Hn2A6iA/YnKzik0ttRlUPxsnvczUIgZQ+8gfySotYSXF6v2/7+pC9T/KAFBNMI4OAHwgfQCAD6QPAPCB9AEAPpA+AMAH0gcA+ED6AAAfSB8A4APpAwB8IH0AgA+kDwDwgX9kl2/W7KmBU0YxAKg2qPv87afd22/fuRH09bc03a6dT1GRhAFAtUH6/O3OnZuyaZ+OnzAAqE5KS5+SkpIdO7dERq2h6febNBsyeGSzZu7CrKhN6w4e+jk1NcXa2tbdrcWkiUFicVmLzz/Ad+iQrzIzn9Oz9PT0Wnp4jh0zRVdXzz/AZ/CgEQMHDJOtuZt/B79uvUZ8OS49PW3FyojrN64WFBS0bOk5aOBwR8eyLw6Mj7/3xZd9589dGh4Rampqtm7N1ocPEzZsXBV39bJUKm3atHnf3oOE7Xnw4P7efTuv/HYxKemJs1P9rl39/br1pPKJk0dcvXqFJg4d+u/qVZu3bFmfk5O9OHylgl2gVQ0b3mfFD5HR0RtOnzluZWXdoX1n2kh8izzAm1Bav8+atd/t2bMj5Nvw6dPmWlnZfB00js5/KqcI2L1n+6iRE3fuOPjFsNHHTxymkBKeoqWlFRMTRafx7p+ORG6IvXY9bmPkagMDA882H586dVS25kuXz+fl5fl07EIxNClwJAXKpInT1q+LMTM1Hz1mcOKTx8Kq6HfU5nV9en8eOHm6RCKhNKEUWBD23eJFKzU1NIOnT6LAomV+WLH44sVfJ4z/Omz+coqeZcsXnDt/hsqXRqxp0sS1c+d/HTtyqVHDxuV3raJdEP7o4ohQH58uhw78GhwUun3H5mPHDzMAeAPKqftkZmXSiTdxwjctPdrQw9at2+bl5aalp5qZW2zdFjnqq0kffdSeytt7+8bH/7F5y48B3fsKp669veNfdRxDI6r73L17iya9vX1D5wY/TXpiZ1uHHp4+fczZub6LS8O4uMuUaFQf+fCDllQ+6quJZ86eiI2NHj9uqkhU9t1Y9Nd79RxAE/fv/5GRkd4joJ+QI7Nmhl39/UpxcTFNz5gxn7ZNWPMH7h4HDuy9cPFsm9ZtK9q17JzsinZBWMC7nS8V0oSb24d17OxpF3x9ujAAeB3lpE/Cg/v0u3Hjpn+tVFMz5NtFNHHz1vWioiKqU8iWbNSoSU5OTmLiIwoU4aFslpGRcW5uDk209fLW0dGh6k/vXgOp3XTi5BGaoHKqHFFmCdHDyr6WT0SNIIqVv1fe8K+1OTjUpfZX2MLZnXy70jKurm4UNH8tJJXu2rXt/IUzjx79KRTY2dkr2DVarKJdoN18aRcMDY2ovcYA4A0oJ32EU05XR/el8vT01JfK9fT06Xd+fp7wUKizvERXV9fLs92p08codK5di8vOzqIQEf4KBUEHH4/yC1PKyKa1X3yVO6HwWrZk7X/3794ZG/3j+hV16jgMGTSiU6eupaWl30ybQINZXw4f6+7uYWRoNG7CF0whBbtAcUkTQh8WALwt5aSPgUHZN1JRi0ZueX5BvqxEWMbc3FLxCtu37zRr9tS0tNSTp45Sn7GNjS0VWlhYUuf03NAl5ZfUEMvv4q1b15maZtSrfeXKhV8O7J0XNtPJuT6lz+3bN8IXrWjxYSthMUo0K0tr9rpdk7sLGJIHqArlXLcbNHiPmiGyRhA1l6iKcfDgzy4ujajr98aNq7Ilb926TjUOGh5SvELqeKbu53PnTx89dpD6m4VCWlt+fj6NOlEzSvixsbGjP/3q06l7iBKHCdUor3azZy2gzaMeGRpfo0JZ3CQkxNOP4i2p9C4AgGLKSR9DQ0NqHNGYF53zv8Vd+u77RZcvn6e+EmMjYyrfvGX92bMns7KzaDD7p90xPXsOeG1rhfp3vLy89+7dSXkh9OkSqrC0auUVHj4nOTmJynfv2fHVqM8PvEiZl2RlZS5cFLJy1dLHiY+o42ZL9AbqcnZt6kZD7BRDMds30cZQQtF2Ukd1UvJT4VnUBU7JQoPx1GMtW1WldwEAFFPa531oDHvpsrDFEXNpXLyBS6OQ2Yuo7UPlY0YH0ok6Z+40Ov+p/6V/v6H9+g5+kxW2b+cbfHgypYOZmbmscP7cpXv3xYaEBt28ec3R0cnX99OAgL6vPpe6mSdPmkbj9zQSRw89WrSOWLxK6OcOnhYaGbXGz78jZU1w0BwamJsxc8rgoT0jN+z87F8BVD/6z9QxNE5ffm2V3gUAUED+97hfOJguKWBu7c0ZVLP9Pz72DrC0ddZlALUM/tMCAPhA+gAAH0gfAOAD6QMAfCB9AIAPpA8A8IH0AQA+kD4AwAfSBwD4QPoAAB9IHwDgA+kDAHwgfQCAD/npo6uvUVpSyqD6GZlpamiKGEDtI/8WWSaWmk8T8hlUv/jfc6wcdBhA7SM/fRwa6kvySxhUsycP8hq3MmIAtZL89KG2QOsu5oeiEhlUm/zc4lOxyR164/7QUEvJv7ehIPF+/sGoJHdvc1MbHT1D9E8rh1jMMlIkOc+L4o6lfx5cV0cPX7sMtZSi9CE5z4uvHM1ISijIz1b1hlipVFpUVKSjrc1Um4mlFtU4HRrqefjixrVQq70mfdRIQkJCYGBgbGwsAwB1gPYUAPCB9AEAPpA+AMAH0gcA+ED6AAAfSB8A4APpAwB8IH0AgA+kDwDwgfQBAD6QPgDAB9IHAPhA+gAAH0gfAOAD6QMAfCB9AIAPpA8A8IH0AQA+kD4AwAfSBwD4QPoAAB9IHwDgA+kDAHzUnPQRiUT169dnAKAmak76SKXS+Ph4BgBqAi0vAOAD6QMAfCB9AIAPpA8A8IH0AQA+kD4AwAfSBwD4QPoAAB9IHwDgA+kDAHwgfQCAD6QPAPCB9AEAPpA+AMAH0gcA+BBJpVKmzkaOHJmbmysWiwsKCh49euTi4kLThYWFMTExDABUmNrXfTw8PFavXi17ePv2bfptbW3NAEC1iZma69u3r6OjY/kSqs25u7szAFBtap8+RkZGXbt2FYlEshI7O7t+/foxAFBtap8+pE+fPg4ODrKHzZs3b9asGQMA1VYT0sfY2JiqP8I0VXz69+/PAEDl1YT0IdTUcnJyognXFxgAqDzlj3llpRWJxCL2rul27dxj9+7dAd0GZGcUs3eO+p0MTfHhKYC3oLTP+zyJz79yNCPhRp5dfb2c9CJWy1jU0aFXoIG7YbsAS02tGlKjBKhWykmfP2/lnduf1tbPxthSq/zwU60iKShJTyo8vOnJFyH1dPQ1GAAopIT0SbiZe/FQRpehDgxefNooKuT+2IgGDAAUUkIb4bdjz30G1GHwAlX9OvSxPbU7lQGAQlVNn8y0Iupm1tJGT8ffjC20/7yVywBAoaqmxvNnRfYN9RmUY2qlTf0+6v7vuwDVraqDxNJSlpPJYYRbxSUnFNTa3neAN4SPqAAAH0gfAOAD6QMAfCB9AIAPpA8A8IH0AQA+kD4AwAfSBwD4QPoAAB9IHwDgA+kDAHzUrv9NH/pF76XLwhgAqADUfQCAD6QPAPChNulTXFz84/oV586fTklJcnV17+7Xu02bj4RZ/gG+Q4d8lZn5PDJqjZ6eXksPz7FjplhYWNKshIT4sAWz/nz4wN3dY9DA4QwAVIba9Pss/27hztjo7v59orfs827nM+vbqSdOHhFmaWlpxcREicXi3T8didwQe+163MbI1VReVFT0ddA4Kyubjet3jvxy/LaYqLQ03PAUQFWoR/oUFhYePPRz/35Dun3Ww8TYpOunfj4du0RtWitbwN7eceCAYUaGRlTlobrP3bu3qPDkqaMpKcljRgfa2Ng6O9cfP25qTk42AwDVoB7pQ2kikUgoVmQl7m4t4uPvZWZlCg8bNWoim2VkZJybm0MTiYmPdHV1bW3thHIKJmtrGwYAqkE9+n2EOsu4CV+8VJ6RnkZVIfbimyRefVZWVqae3j/uOa2jo8sAQDWoR/pYWFrR78DJwdTCKl9ubW2r4FnGxib5+XnlS/Ly8FUTAKpCPdLHwb6ujo4OTXzg7iGUZGSkS6VSfX1FX6dha2NXUFBADbT69cu+2+/evbupqc8YAKgG9ej3oZQZMngkdTNfuxZHHUA02jVl6ujXfmrZy8tbW1s7PCKUMohyJyQ0yPhFMw0AVIHafN6nb59BLi6NordtvHLlgoGBYdP3mwcGTlf8FENDw3lzl65Zs/zf3byp+3nEl+P/78gvDABUQ1W/xz3hZl7cyec+/fBNyv8QOfve2CX4KncARfCfFgDAx7tOn8Apo4SPAr6kpKREyqSaGvK3Z/Om3SYmpkxJordu3Lp1o/x5NHJfQWVw3dptNjaKhtgA4K286/SZFjRHUiSRO6uwsFAY2HqVEqOHfPZZjw4dOsudlZ2VZWRsLHeW8I9jAKAs7zp9VOEcNjI0oh+5s+xs0YEF8I6g3wcA+ED6AAAfSB8A4APpAwB8IH0AgA+kDwDwgfQBAD6QPgDAB9IHAPio6v19RGKpoYkWg3+yq69XxZsHANR4VU0fcxvtR3dwu9J/yEguLMwrkXuraQCQqWr6GJlpWdhpF+SVMPifzGcS56b6DAAUUsKdVVt2Nju8KZHBC3lZRWf3pXj9G/8QD/AaIqV0T6Q8LDiwKcmrm42JpbauvgarlbIziqjNdSo2eXhoPU1ttfmSWABeRMrqHM1Illz6v4yEm7nG5lqZaUWslrF21M1Mlbi4GXzUzYoBwBsQKX1opiC3VFQLL/xSqU5trfQBVI4IA8MAwAU+bQgAfCB9AIAPpA8A8IH0AQA+kD4AwAfSBwD4+H8AAAD///WWQnwAAAAGSURBVAMAU06fnBq+IlIAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# LLM\n",
    "# model = ChatOpenAI(model=\"gpt-4o\", temperature=0) \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\" ,temperature=0)\n",
    "\n",
    "# State \n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State, config: RunnableConfig):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages, config)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State):\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f847a787-b301-488c-9b58-cba9f389f55d",
   "metadata": {},
   "source": [
    "### Streaming full state\n",
    "\n",
    "Now, let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "`.stream` and `.astream` are sync and async methods for streaming back results. \n",
    " \n",
    "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
    " \n",
    "* `values`: This streams the full state of the graph after each node is called.\n",
    "* `updates`: This streams updates to the state of the graph after each node is called.\n",
    "\n",
    "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
    "\n",
    "Let's look at `stream_mode=\"updates\"`.\n",
    "\n",
    "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
    "\n",
    "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation': {'messages': AIMessage(content=\"Hi Lance! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-a0026c08-2028-4f1b-94fb-e8802a643060-0', usage_metadata={'input_tokens': 6, 'output_tokens': 19, 'total_tokens': 25, 'input_token_details': {'cache_read': 0}})}}\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88999cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is the weather like in there?', additional_kwargs={}, response_metadata={}, id='52d4c399-9d74-49cd-87bf-bc51cc09bc7f'), AIMessage(content='Since I don\\'t have a physical location, \"in here\" doesn\\'t really apply to me. I exist as code on servers distributed across various locations.\\n\\nTo give you a helpful answer, I need to know what location you\\'re interested in. Could you tell me the city and state (or country) you\\'d like to know the weather for?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-619f2372-4312-4c58-9e90-001516645f2a-0', usage_metadata={'input_tokens': 148, 'output_tokens': 76, 'total_tokens': 224, 'input_token_details': {'cache_read': 0}})], 'summary': 'The conversation started with Lance introducing himself. I responded in kind and offered assistance. Lance then asked about the weather \"in there,\" which I interpreted as asking about the weather where I am. I explained that as a language model, I don\\'t have a physical location or experience weather. I then offered to provide weather information for a specific location if Lance provided it. Lance then repeated the same question. I reiterated that I don\\'t have a physical location and need a specific location to provide weather information.'}\n"
     ]
    }
   ],
   "source": [
    "# Now with .invoke() NO MORE `node_name` as the key and the updated state as the value.\n",
    "# Only the final state is returned.\n",
    "response = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the weather like in there?\")],},\n",
    "    config,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
   "metadata": {},
   "source": [
    "Let's now just print the state update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c859c777-cb12-4682-9108-6b367e597b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Lance! Nice to meet you again. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    chunk['conversation'][\"messages\"].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583bf219-6358-4d06-ae99-c40f43569fda",
   "metadata": {},
   "source": [
    "Now, we can see `stream_mode=\"values\"`.\n",
    "\n",
    "This is the `full state` of the graph after the `conversation` node is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Lance\n",
      "---------------------------------------------------------------------------\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Lance\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Lance! It's nice to meet you. How can I help you today?\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    for m in event['messages']:\n",
    "        m.pretty_print()\n",
    "    print(\"---\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
   "metadata": {},
   "source": [
    "### Streaming tokens\n",
    "\n",
    "We often want to stream more than graph state.\n",
    "\n",
    "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
    "\n",
    "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
    "\n",
    "Each event is a dict with a few keys:\n",
    " \n",
    "* `event`: This is the type of event that is being emitted. \n",
    "* `name`: This is the name of event.\n",
    "* `data`: This is the data associated with the event.\n",
    "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: . Type: on_chain_start. Name: LangGraph\n",
      "Node: conversation. Type: on_chain_start. Name: conversation\n",
      "Node: conversation. Type: on_chat_model_start. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_end. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chain_start. Name: should_continue\n",
      "Node: conversation. Type: on_chain_end. Name: should_continue\n",
      "Node: conversation. Type: on_chain_stream. Name: conversation\n",
      "Node: conversation. Type: on_chain_end. Name: conversation\n",
      "Node: . Type: on_chain_stream. Name: LangGraph\n",
      "Node: . Type: on_chain_end. Name: LangGraph\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
   "metadata": {},
   "source": [
    "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
    "\n",
    "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
    "\n",
    "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk': AIMessageChunk(content='Alright', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 11, 'output_tokens': 0, 'total_tokens': 11, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=\", let's dive into the San Francisco 49ers! Here'\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='s a breakdown of the team, covering their history, notable players, recent performance, and more', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=':\\n\\n**General Overview:**\\n\\n*   **Team Name:** San Francisco 49ers (often shortened to \"Niners\" or \"49ers\")', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='\\n*   **City:** Santa Clara, California (though they represent the San Francisco Bay Area)\\n*   **Conference:** National Football Conference (NFC)', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=\"\\n*   **Division:** NFC West\\n*   **Stadium:** Levi's Stadium\\n*   **Team Colors:** Red, Gold, and White\\n*   **Mascot:** Sourdough Sam\\n*   **Owner\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=':** Denise DeBartolo York and John York\\n*   **Head Coach:** Kyle Shanahan\\n*   **General Manager:** John Lynch\\n\\n**A Storied History:**\\n\\n*   **Founded:** 1946 as part of the All', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='-America Football Conference (AAFC). They joined the NFL in 1950 when the leagues merged.\\n*   **Early Years:** The 49ers had some success in their early NFL years, but true dominance came later.\\n*   **Bill Walsh Era (1979-19', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='88):** This is arguably the most important period in 49ers history. Head Coach Bill Walsh implemented the \"West Coast Offense,\" a precision passing attack that revolutionized the game.\\n*   **Dynasty Years (1980s-1990s):** Under Walsh and later George', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=' Seifert, the 49ers won five Super Bowls:\\n    *   Super Bowl XVI (1981 season) - Defeated the Cincinnati Bengals\\n    *   Super Bowl XIX (1984 season) - Defeated the Miami Dolphins\\n    *   Super Bowl XXIII (198', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='8 season) - Defeated the Cincinnati Bengals\\n    *   Super Bowl XXIV (1989 season) - Defeated the Denver Broncos\\n    *   Super Bowl XXIX (1994 season) - Defeated the San Diego Chargers\\n*   **Post-Dynasty:** After', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=' the 1990s, the 49ers had periods of success but struggled to consistently contend for Super Bowls.\\n*   **Jim Harbaugh Era (2011-2014):** Harbaugh brought the 49ers back to prominence, leading them to Super Bowl XLVII', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=' (2012 season), where they lost to the Baltimore Ravens.\\n*   **Kyle Shanahan Era (2017-Present):** Shanahan has rebuilt the 49ers into a consistent contender. They reached Super Bowl LIV (2019 season), losing to the Kansas City Chiefs. They', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=' also reached Super Bowl LVIII (2023 season), losing to the Kansas City Chiefs.\\n\\n**Key Players (Past and Present):**\\n\\n*   **Joe Montana (QB):** Widely considered one of the greatest quarterbacks of all time. Led the 49ers to four Super Bowl victories.', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=\"\\n*   **Jerry Rice (WR):** Arguably the greatest wide receiver in NFL history. Holds numerous NFL records.\\n*   **Steve Young (QB):** Montana's successor, led the 49ers to Super Bowl XXIX victory and won three Super Bowl rings as a backup to Montana.\\n*\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=\"   **Ronnie Lott (S):** A hard-hitting safety and a key member of the 49ers' dominant defense in the 1980s.\\n*   **George Kittle (TE):** One of the best tight ends in the league, known for his blocking and receiving abilities\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='.\\n*   **Christian McCaffrey (RB):** Star running back acquired in 2022, a dynamic player in both the running and passing game.\\n*   **Nick Bosa (DE):** Dominant defensive end and former Defensive Player of the Year.\\n*   **Brock Pur', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='dy (QB):** The current starting quarterback, drafted last overall in 2022, nicknamed \"Mr. Irrelevant\".\\n\\n**Recent Performance (Last Few Years):**\\n\\n*   **2019 Season:** Reached Super Bowl LIV, losing to the Kansas City Chiefs.\\n*   **', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='2020 Season:** Injuries plagued the team, and they finished with a losing record.\\n*   **2021 Season:** Reached the NFC Championship Game, losing to the Los Angeles Rams.\\n*   **2022 Season:** Reached the NFC Championship Game, losing to the Philadelphia Eagles.', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='\\n*   **2023 Season:** Reached Super Bowl LVIII, losing to the Kansas City Chiefs.\\n\\n**Strengths:**\\n\\n*   **Strong Coaching:** Kyle Shanahan is considered one of the best offensive minds in the NFL.\\n*   **Talented Roster:** The 49ers have', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=\" a roster filled with Pro Bowl-caliber players on both sides of the ball.\\n*   **Versatile Offense:** Shanahan's offensive scheme is known for its creativity and ability to adapt to different opponents.\\n*   **Dominant Defense:** The 49ers' defense is consistently one of the best in\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=' the league, led by star players like Nick Bosa.\\n\\n**Weaknesses:**\\n\\n*   **Quarterback Consistency:** While Brock Purdy has shown promise, the quarterback position has been a question mark at times.\\n*   **Super Bowl Heartbreak:** The team has struggled to win the big game, losing two', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=' Super Bowls in the Shanahan era.\\n\\n**Overall:**\\n\\nThe San Francisco 49ers are one of the most iconic and successful franchises in NFL history. They have a rich tradition, a passionate fanbase, and a talented team that is consistently in contention for championships. They are a team to watch in the NFC.', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': []}, id='run-fd1f2892-083b-44e0-be85-d6921825425f', usage_metadata={'input_tokens': -1, 'output_tokens': 1222, 'total_tokens': 1221, 'input_token_details': {'cache_read': 0}})}\n"
     ]
    }
   ],
   "source": [
    "node_to_stream = 'conversation'\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        print(event[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
   "metadata": {},
   "source": [
    "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay|, let'|s break down the San Francisco 49ers:\n",
      "\n",
      "**In a Nutshell:**| The San Francisco 49ers are a professional American football team based in Santa Clara,| California (representing the San Francisco Bay Area). They are members of the National Football Conference (NFC) West division in the National Football League (NFL). The| 49ers are one of the most storied and successful franchises in NFL history.\n",
      "\n",
      "**Key Facts:**\n",
      "\n",
      "*   **Founded:** 1946 (originally| in the All-America Football Conference, joined the NFL in 1950)\n",
      "*   **Home Stadium:** Levi's Stadium (Santa Clara, CA)\n",
      "*   **Team Colors:** Red, Gold, and White|\n",
      "*   **Mascot:** Sourdough Sam\n",
      "*   **Owner:** Denise DeBartolo York and John York\n",
      "*   **Head Coach:** Kyle Shanahan\n",
      "*   **General Manager:** John Lynch\n",
      "*   **Super| Bowl Wins:** 5 (XVI, XIX, XXIII, XXIV, XXIX)\n",
      "\n",
      "**Historical Significance:**\n",
      "\n",
      "*   **The Dynasty Era (1980s-1990s):** The 49ers were a dominant force in the NFL, largely due to the \"West Coast Offense|\" pioneered by coach Bill Walsh. This era saw them win four Super Bowls with legendary quarterback Joe Montana and wide receiver Jerry Rice.\n",
      "*   **Steve Young Era:** After Montana, Steve Young continued the success, leading the team to another Super Bowl victory in 1994.\n",
      "*   **Legacy:** The 4|9ers are known for their innovative offensive schemes, strong quarterback play, and a winning tradition.\n",
      "\n",
      "**Key Players (Past and Present):**\n",
      "\n",
      "*   **Joe Montana (QB):** Arguably one of the greatest quarterbacks of all time.\n",
      "*   **Jerry Rice (WR):** Widely considered the greatest| wide receiver in NFL history.\n",
      "*   **Steve Young (QB):** A Hall of Fame quarterback who succeeded Montana.\n",
      "*   **Ronnie Lott (S):** A legendary safety known for his toughness and leadership.\n",
      "*   **George Kittle (TE):** One of the best tight ends in the league| today.\n",
      "*   **Christian McCaffrey (RB):** A dynamic running back and receiving threat.\n",
      "*   **Nick Bosa (DE):** A dominant pass rusher and defensive leader.\n",
      "*   **Brock Purdy (QB):** The current starting quarterback, drafted last overall in 2022|.\n",
      "\n",
      "**Recent Performance:**\n",
      "\n",
      "*   **Kyle Shanahan Era:** Since becoming head coach in 2017, Shanahan has rebuilt the 49ers into a consistent contender.\n",
      "*   **Super Bowl Appearances:** They reached Super Bowl LIV (2019 season) and Super Bowl| LVIII (2023 season), losing both times to the Kansas City Chiefs.\n",
      "*   **Consistent Playoff Contender:** The 49ers have been a regular playoff team in recent years, showcasing their talent and coaching.\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "*   **Offensive Scheme:** Kyle Shanahan's| offensive system is known for its creativity and effectiveness.\n",
      "*   **Talented Roster:** The 49ers have a well-rounded roster with stars at key positions.\n",
      "*   **Strong Defense:** Their defense is consistently ranked among the best in the league.\n",
      "*   **Running Game:** With Christian McCaffrey|, they have a potent running attack.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "*   **Super Bowl Struggles:** Despite being a strong team, they have struggled to win the Super Bowl in recent appearances.\n",
      "*   **Consistency:** While generally consistent, they can have occasional lapses in performance.\n",
      "\n",
      "**In Conclusion:** The San Francisco 4|9ers are a team with a rich history, a strong present, and a bright future. They are a perennial contender in the NFC and a team that fans across the league enjoy watching. They are known for their exciting play, talented players, and passionate fanbase.|"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        data = event[\"data\"]\n",
    "        print(data[\"chunk\"].content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b9d80e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation': {'messages': AIMessage(content='Okay, let\\'s break down the San Francisco 49ers:\\n\\n**Franchise Overview:**\\n\\n*   **Team Name:** San Francisco 49ers (often shortened to \"49ers\" or \"Niners\")\\n*   **Location:** Santa Clara, California (representing the San Francisco Bay Area)\\n*   **Conference:** National Football Conference (NFC)\\n*   **Division:** NFC West\\n*   **Stadium:** Levi\\'s Stadium\\n*   **Team Colors:** Red, Gold, and White\\n*   **Mascot:** Sourdough Sam\\n*   **Owner:** Denise DeBartolo York, Jed York (Co-Chairmen)\\n*   **Head Coach:** Kyle Shanahan\\n*   **General Manager:** John Lynch\\n\\n**History and Success:**\\n\\n*   **Founded:** 1946 (as part of the All-America Football Conference - AAFC, joined the NFL in 1950)\\n*   **Super Bowl Championships:** 5 (XVI, XIX, XXIII, XXIV, XXIX)\\n*   **NFC Championships:** 8\\n*   **Division Championships:** 21\\n*   **Dynasty Era (1980s-1990s):** The 49ers were the dominant team in the NFL, led by legendary coach Bill Walsh and quarterbacks Joe Montana and Steve Young. This era is known for its innovative offensive schemes and consistent success.\\n\\n**Key Players (Past and Present):**\\n\\n*   **Hall of Famers:**\\n    *   **Joe Montana (QB):** Considered one of the greatest quarterbacks of all time.\\n    *   **Jerry Rice (WR):** Widely regarded as the greatest wide receiver in NFL history.\\n    *   **Steve Young (QB):** Montana\\'s successor, also a Hall of Famer.\\n    *   **Ronnie Lott (S):** A hard-hitting and impactful safety.\\n    *   **Steve Young (QB):** Hall of Fame Quarterback who took over for Joe Montana.\\n    *   **Bill Walsh (Coach):** Revolutionary coach who developed the \"West Coast Offense.\"\\n    *   **John Lynch (S):** Current GM, but also a Hall of Fame Safety who played for the Buccaneers and Broncos.\\n*   **Current Stars:**\\n    *   **Brock Purdy (QB):** The current starting quarterback, drafted last overall in 2022.\\n    *   **Christian McCaffrey (RB):** A dynamic running back known for his receiving skills.\\n    *   **Deebo Samuel (WR/RB):** A versatile player who can line up at multiple positions.\\n    *   **George Kittle (TE):** One of the best tight ends in the league, known for his blocking and receiving.\\n    *   **Nick Bosa (DE):** A dominant pass rusher and former Defensive Player of the Year.\\n    *   **Fred Warner (LB):** A key leader and playmaker on the defense.\\n\\n**Recent Performance:**\\n\\n*   **2019 Season:** Reached Super Bowl LIV, losing to the Kansas City Chiefs.\\n*   **2021 Season:** Reached the NFC Championship Game, losing to the Los Angeles Rams.\\n*   **2022 Season:** Reached the NFC Championship Game, losing to the Philadelphia Eagles.\\n*   **2023 Season:** Reached Super Bowl LVIII, losing to the Kansas City Chiefs in overtime.\\n\\n**Coaching and Management:**\\n\\n*   **Kyle Shanahan (Head Coach):** Known for his offensive play-calling and innovative schemes. He\\'s been the head coach since 2017.\\n*   **John Lynch (General Manager):** A former NFL safety, Lynch has been the GM since 2017 and has been instrumental in building the current roster.\\n\\n**Key Strengths:**\\n\\n*   **Offensive Scheme:** Shanahan\\'s offensive system is known for its versatility, play-action passing, and emphasis on the running game.\\n*   **Defensive Line:** The 49ers consistently have a strong defensive line, led by players like Nick Bosa.\\n*   **Playmakers:** The team has a number of players who can make big plays on offense and defense.\\n*   **Strong Roster:** The 49ers have built a deep and talented roster through the draft and free agency.\\n\\n**Things to Know:**\\n\\n*   **\"The Catch\":** One of the most famous plays in NFL history, Joe Montana\\'s pass to Dwight Clark in the 1981 NFC Championship Game.\\n*   **Passionate Fanbase:** The 49ers have a large and dedicated fanbase in the Bay Area and beyond.\\n*   **Consistent Contenders:** Under Shanahan and Lynch, the 49ers have been consistently competitive and a threat to make deep playoff runs.\\n\\nIn short, the San Francisco 49ers are a storied franchise with a rich history, a talented roster, and a coaching staff dedicated to winning. They are a team that is always in the conversation as a Super Bowl contender.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-123774e9-f81f-4920-a74b-d3568b4c61e4-0', usage_metadata={'input_tokens': 1152, 'output_tokens': 1078, 'total_tokens': 2230, 'input_token_details': {'cache_read': 0}})}}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"52\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream({\"messages\": [input_message]}, config,):\n",
    "    # Get chat model tokens from a particular node \n",
    "    print(event)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
   "metadata": {},
   "source": [
    "### Streaming with LangGraph API\n",
    "\n",
    "** DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "-  API: http://127.0.0.1:2024\n",
    "-  Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "-  API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.\n",
    "\n",
    "The LangGraph API [supports editing graph state](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/#initial-invocation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "079c2ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'assistant_id': '6f6fce9a-b777-529d-9699-dd340ddec86c',\n",
       "  'graph_id': 'dynamic_breakpoints',\n",
       "  'config': {},\n",
       "  'metadata': {'created_by': 'system'},\n",
       "  'name': 'dynamic_breakpoints',\n",
       "  'created_at': '2025-05-28T02:09:06.249391+00:00',\n",
       "  'updated_at': '2025-05-28T02:09:06.249391+00:00',\n",
       "  'version': 1,\n",
       "  'description': None},\n",
       " {'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca',\n",
       "  'graph_id': 'agent',\n",
       "  'config': {},\n",
       "  'metadata': {'created_by': 'system'},\n",
       "  'name': 'agent',\n",
       "  'created_at': '2025-05-28T02:09:06.240380+00:00',\n",
       "  'updated_at': '2025-05-28T02:09:06.240380+00:00',\n",
       "  'version': 1,\n",
       "  'description': None}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "# This is the URL of the local development server\n",
    "URL = \"http://127.0.0.1:2024\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()\n",
    "assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32",
   "metadata": {},
   "source": [
    "Let's [stream `values`](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/), like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamPart(event='metadata', data={'run_id': '1f03b68d-4792-6d60-af92-916f185eb077', 'attempt': 1})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '0ffce85f-f70d-45c7-9934-1ab5f28bc75b', 'example': False}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '0ffce85f-f70d-45c7-9934-1ab5f28bc75b', 'example': False}, {'content': '', 'additional_kwargs': {'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run-f242889e-2418-4b7b-9e5a-888dd677f293-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': '763153fc-3403-49ad-95a9-975c2606c471', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 74, 'output_tokens': 5, 'total_tokens': 79, 'input_token_details': {'cache_read': 0}}}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '0ffce85f-f70d-45c7-9934-1ab5f28bc75b', 'example': False}, {'content': '', 'additional_kwargs': {'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run-f242889e-2418-4b7b-9e5a-888dd677f293-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': '763153fc-3403-49ad-95a9-975c2606c471', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 74, 'output_tokens': 5, 'total_tokens': 79, 'input_token_details': {'cache_read': 0}}}, {'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'd570c4c1-aa66-4fe6-88c1-acd22c41096c', 'tool_call_id': '763153fc-3403-49ad-95a9-975c2606c471', 'artifact': None, 'status': 'success'}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '0ffce85f-f70d-45c7-9934-1ab5f28bc75b', 'example': False}, {'content': '', 'additional_kwargs': {'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run-f242889e-2418-4b7b-9e5a-888dd677f293-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': '763153fc-3403-49ad-95a9-975c2606c471', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 74, 'output_tokens': 5, 'total_tokens': 79, 'input_token_details': {'cache_read': 0}}}, {'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'd570c4c1-aa66-4fe6-88c1-acd22c41096c', 'tool_call_id': '763153fc-3403-49ad-95a9-975c2606c471', 'artifact': None, 'status': 'success'}, {'content': 'The product of 2 and 3 is 6.', 'additional_kwargs': {}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run-805f0d7f-4d6d-4c59-a7f2-4f9cc50785e2-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 82, 'output_tokens': 12, 'total_tokens': 94, 'input_token_details': {'cache_read': 0}}}]})\n"
     ]
    }
   ],
   "source": [
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "# Input message\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
   "metadata": {},
   "source": [
    "The streamed objects have: \n",
    "\n",
    "* `event`: Type\n",
    "* `data`: State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57b735aa-139c-45a3-a850-63519c0004f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "content='Multiply 2 and 3' additional_kwargs={} response_metadata={} id='5b63a534-9362-45d5-9dcd-bbe482653f97'\n",
      "=========================\n",
      "content='' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 74, 'output_tokens': 5, 'total_tokens': 79, 'input_token_details': {'cache_read': 0}}, 'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-324eaf43-e906-488d-ab0b-cff961ca120a-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': '1ac4b6d6-941b-45b7-ad0e-7c5fe534fa3c', 'type': 'tool_call'}]\n",
      "=========================\n",
      "content='6' name='multiply' id='5f09c4e2-5a70-4120-986f-1dfce4b95d23' tool_call_id='1ac4b6d6-941b-45b7-ad0e-7c5fe534fa3c'\n",
      "=========================\n",
      "content='The product of 2 and 3 is 6.' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 82, 'output_tokens': 12, 'total_tokens': 94, 'input_token_details': {'cache_read': 0}}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-2382a513-89b7-48cd-9352-d1b5fc3cc145-0'\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    messages = event.data.get('messages',None)\n",
    "    if messages:\n",
    "        print(convert_to_messages(messages)[-1])\n",
    "    print('='*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05177701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "[AIMessage(content='', additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 74, 'output_tokens': 5, 'total_tokens': 79, 'input_token_details': {'cache_read': 0}}, 'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-f680f4a1-e6fa-4abb-89f1-f04af8218271-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': '07b5476c-7b10-4b87-b299-a9fb2d31f26d', 'type': 'tool_call'}])]\n",
      "[{'content': '', 'additional_kwargs': {'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run-f680f4a1-e6fa-4abb-89f1-f04af8218271-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': '07b5476c-7b10-4b87-b299-a9fb2d31f26d', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 74, 'output_tokens': 5, 'total_tokens': 79, 'input_token_details': {'cache_read': 0}}}]\n",
      "=========================\n",
      "=========================\n",
      "[AIMessage(content='The result of multiplying 2 and 3 is 6.', additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 82, 'output_tokens': 13, 'total_tokens': 95, 'input_token_details': {'cache_read': 0}}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-50851a5d-29da-4a7a-8487-780f14584b2a-0')]\n",
      "[{'content': 'The result of multiplying 2 and 3 is 6.', 'additional_kwargs': {}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run-50851a5d-29da-4a7a-8487-780f14584b2a-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 82, 'output_tokens': 13, 'total_tokens': 95, 'input_token_details': {'cache_read': 0}}}]\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"updates\"):\n",
    "    messages = event.data.get('assistant',None)\n",
    "    if messages:\n",
    "        # print(convert_to_messages(messages[\"messages\"]))\n",
    "        print(messages[\"messages\"])\n",
    "    # print(event.data)\n",
    "    # print(event)\n",
    "    print('='*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555d186-27be-4ddf-934c-895a3105035d",
   "metadata": {},
   "source": [
    "There are some new streaming mode that are only supported via the API.\n",
    "\n",
    "For example, we can [use `messages` mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) to better handle the above case!\n",
    "\n",
    "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
    "\n",
    "All events emitted using `messages` mode have two attributes:\n",
    "\n",
    "* `event`: This is the name of the event\n",
    "* `data`: This is data associated with the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"messages\"):\n",
    "    print(event.event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
   "metadata": {},
   "source": [
    "We can see a few events: \n",
    "\n",
    "* `metadata`: metadata about the run\n",
    "* `messages/complete`: fully formed message \n",
    "* `messages/partial`: chat model tokens\n",
    "\n",
    "You can dig further into the types [here](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages).\n",
    "\n",
    "Now, let's show how to stream these messages. \n",
    "\n",
    "We'll define a helper function for better formatting of the tool calls in messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Run ID - 1f03b6e8-ae50-67e6-878d-32abd78c2ed5\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: 4e8eb6c0-5154-4669-9c3b-3cdda3bd3ccc, Function: multiply, Arguments: {'a': 2.0, 'b': 3.0}\n",
      "Response Metadata: Finish Reason - STOP\n",
      "--------------------------------------------------\n",
      "AI: The\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "Response Metadata: Finish Reason - STOP\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "\n",
    "def format_tool_calls(tool_calls):\n",
    "    \"\"\"\n",
    "    Format a list of tool calls into a readable string.\n",
    "\n",
    "    Args:\n",
    "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
    "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tool_calls:\n",
    "        formatted_calls = []\n",
    "        for call in tool_calls:\n",
    "            formatted_calls.append(\n",
    "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
    "            )\n",
    "        return \"\\n\".join(formatted_calls)\n",
    "    return \"No tool calls\"\n",
    "\n",
    "async for event in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input={\"messages\": [input_message]},\n",
    "    stream_mode=\"messages\",):\n",
    "    \n",
    "    # Handle metadata events\n",
    "    if event.event == \"metadata\":\n",
    "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Handle partial message events\n",
    "    elif event.event == \"messages/partial\":\n",
    "        for data_item in event.data:\n",
    "            # Process user messages\n",
    "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
    "                print(f\"Human: {data_item['content']}\")\n",
    "            else:\n",
    "                # Extract relevant data from the event\n",
    "                tool_calls = data_item.get(\"tool_calls\", [])\n",
    "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
    "                content = data_item.get(\"content\", \"\")\n",
    "                response_metadata = data_item.get(\"response_metadata\", {})\n",
    "\n",
    "                if content:\n",
    "                    print(f\"AI: {content}\")\n",
    "\n",
    "                if tool_calls:\n",
    "                    print(\"Tool Calls:\")\n",
    "                    print(format_tool_calls(tool_calls))\n",
    "\n",
    "                if invalid_tool_calls:\n",
    "                    print(\"Invalid Tool Calls:\")\n",
    "                    print(format_tool_calls(invalid_tool_calls))\n",
    "\n",
    "                if response_metadata:\n",
    "                    finish_reason = response_metadata.get(\"finish_reason\", \"N/A\")\n",
    "                    print(f\"Response Metadata: Finish Reason - {finish_reason}\")\n",
    "                    \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
